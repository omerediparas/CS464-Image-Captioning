{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Image Caption Generator","metadata":{"id":"GQnm12NRwzB1"}},{"cell_type":"code","source":"!pip install torch==2.0.0\n!pip install torchvision==0.15.1\n!pip install torchtext==0.15.1\n!pip install transformers\n!pip install datasets\n!pip install opencv-python-headless\n!pip install fsspec==2024.9.0\n\n!pip3 install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\"\n!git clone \"https://github.com/salaniz/pycocoevalcap.git\"\n!pip install ultralytics\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:10:43.649775Z","iopub.execute_input":"2024-12-21T07:10:43.650151Z","iopub.status.idle":"2024-12-21T07:14:39.350708Z","shell.execute_reply.started":"2024-12-21T07:10:43.650112Z","shell.execute_reply":"2024-12-21T07:14:39.349110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torchvision.datasets import CocoDetection\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nimport os\nfrom PIL import Image\nimport numpy as np\nimport requests\nfrom io import BytesIO\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport cv2\nimport pycocoevalcap\nfrom ultralytics import YOLO\nfrom pycocotools.coco import COCO\nfrom pycocoevalcap.eval import COCOEvalCap\nfrom collections import Counter\n# for metrics, COCO API metrics is used\nimport json\nfrom pycocoevalcap import bleu, meteor, rouge, spice\n#from pycocoevalcap.evalcap import COCOEvalCap\n\n# Load YOLOv5 for feature extraction\nimport torch.hub","metadata":{"id":"V1LDt60jwzB7","outputId":"6002c02f-a2b4-4a91-817c-64135274d831","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:39.353413Z","iopub.execute_input":"2024-12-21T07:14:39.353792Z","iopub.status.idle":"2024-12-21T07:14:44.543814Z","shell.execute_reply.started":"2024-12-21T07:14:39.353755Z","shell.execute_reply":"2024-12-21T07:14:44.542731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\n\n# Set the random seed for PyTorch\nseed = 42  # You can choose any integer value\ntorch.manual_seed(seed)\n\n# If you are using GPU (CUDA)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n\n# For Python's random module\nrandom.seed(seed)\n\n# For NumPy\nnp.random.seed(seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:44.545743Z","iopub.execute_input":"2024-12-21T07:14:44.546448Z","iopub.status.idle":"2024-12-21T07:14:44.556668Z","shell.execute_reply.started":"2024-12-21T07:14:44.546396Z","shell.execute_reply":"2024-12-21T07:14:44.555635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setting the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"id":"2-uX4UDUw3Nt","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:44.559551Z","iopub.execute_input":"2024-12-21T07:14:44.559993Z","iopub.status.idle":"2024-12-21T07:14:44.583576Z","shell.execute_reply.started":"2024-12-21T07:14:44.559946Z","shell.execute_reply":"2024-12-21T07:14:44.581796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define transformations for image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"id":"pdEPXli8wzCA","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.641411Z","iopub.execute_input":"2024-12-21T07:14:48.642074Z","iopub.status.idle":"2024-12-21T07:14:48.647744Z","shell.execute_reply.started":"2024-12-21T07:14:48.642038Z","shell.execute_reply":"2024-12-21T07:14:48.646477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vocabulary, will be used later.\nvocabulary = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.649034Z","iopub.execute_input":"2024-12-21T07:14:48.649385Z","iopub.status.idle":"2024-12-21T07:14:48.696491Z","shell.execute_reply.started":"2024-12-21T07:14:48.649350Z","shell.execute_reply":"2024-12-21T07:14:48.695476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"annotations\": [{\"image_id\": 179765,\"id\": 38,\"caption\": \"A black Honda motorcycle parked in front of a garage.\"},...}\nclass CocoDataset(Dataset): # <start> cat sat on the mat <end> -> {0 32 24 34 3 3 1 }\n    def __init__(self, root_dir, annotation_file, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        with open(annotation_file, 'r') as f:\n            self.coco_data = json.load(f)\n        self.tokenizer = get_tokenizer(\"basic_english\")  # Tokenizer from torchtext\n        self.annotations = self.coco_data['annotations']\n        # Build vocabulary\n        self.vocab = self.build_vocab()\n        self.image_id_to_file = {img['id']: img['file_name'] for img in self.coco_data['images']}\n        self.image_id_to_img = {}\n\n\n    def build_vocab(self):\n        counter = Counter()\n        for annotation in tqdm(self.annotations):\n            caption = annotation['caption']\n            tokens = self.tokenizer(caption.lower())\n            counter.update(tokens)\n        vocab = build_vocab_from_iterator([counter], specials=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"])\n        vocab.set_default_index(vocab[\"<unk>\"])  # Out-of-vocabulary words will be mapped to <unk>\n        global vocabulary\n        vocabulary = vocab\n        return vocab\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        image_id = self.annotations[idx]['image_id']\n\n        file_name = self.image_id_to_file.get(image_id)\n        img_path = os.path.join(self.root_dir, file_name)\n\n        try:\n          image = Image.open(img_path).convert('RGB')\n          self.image_id_to_img[image_id] = image # NOTE\n        except Exception as e:\n          print(f\"Error opening image: {file_name}\")\n          raise e\n\n        caption = self.annotations[idx]['caption']\n        tokens = self.tokenizer(caption.lower())\n\n        # Convert caption tokens to indices\n        caption_indices = [self.vocab['<bos>']] + [self.vocab[token] for token in tokens] + [self.vocab['<eos>']]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, {'image_id': image_id,'captions': torch.tensor(caption_indices, dtype=torch.long)}\n","metadata":{"id":"W_cYJu5QwzCB","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.697720Z","iopub.execute_input":"2024-12-21T07:14:48.698050Z","iopub.status.idle":"2024-12-21T07:14:48.715646Z","shell.execute_reply.started":"2024-12-21T07:14:48.698016Z","shell.execute_reply":"2024-12-21T07:14:48.714432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size, model_type = 'resnet50'):\n        \"\"\"\n        EncoderCNN initializes with either ResNet50, VGG16, EfficientNet, or ViT.\n        Args:\n            embed_size (int): The dimension of the embedding space.\n            model_type (str): The type of model ('resnet50', 'vgg16', 'efficientnet', 'vit').\n        \"\"\"\n        super(EncoderCNN, self).__init__()\n        self.model_type = model_type\n\n        if model_type == 'resnet50':\n            self.cnn = models.resnet50(pretrained=True)\n            self.backbone = nn.Sequential(*list(self.cnn.children())[:-1])  # Remove the last FC layer\n            in_features = self.cnn.fc.in_features\n\n        elif model_type == 'vgg16':\n            self.cnn = models.vgg16(pretrained=True)\n            self.backbone = self.cnn.features  # Feature extractor in VGG16\n            in_features = 25088  # Output features of VGG16\n\n        elif model_type == 'efficientnet':\n            self.cnn = models.efficientnet_b0(pretrained=True)  # EfficientNet-B0\n            self.backbone = self.cnn.features  # Feature extractor in EfficientNet\n            in_features = 62720  #self.cnn.classifier[1].in_features  # Feature dimension\n        elif model_type == 'densenet':\n            self.cnn = models.densenet121(pretrained=True)\n            self.backbone = self.cnn.features  # DenseNet feature extractor\n            in_features = 50176  # Output features of DenseNet121\n\n        elif model_type == 'vit':\n            # Initialize a Vision Transformer from Hugging Face\n            vit_config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224\")\n            self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\", config=vit_config)\n           # in_features = 25088  \n            in_features = vit_config.hidden_size  # Output size of ViT embeddings\n\n        else:\n            raise ValueError(f\"Unsupported model_type: {model_type}. Choose 'resnet50', 'vgg16', 'efficientnet', or 'vit'.\")\n\n        # Linear layer to project extracted features to embed_size\n        self.fc = nn.Linear(in_features, embed_size)\n\n    def forward(self, images):\n        \"\"\"\n        Forward pass of the encoder.\n        Args:\n            images (Tensor): Input images (batch_size, 3, H, W).\n        Returns:\n            Tensor: Embedding features of shape (batch_size, embed_size).\n        \"\"\"\n        if self.model_type in ['resnet50', 'vgg16', 'efficientnet','densenet']:\n            with torch.no_grad():\n                features = self.backbone(images)  # Extract features\n            features = features.view(features.size(0), -1)  # Flatten feature maps\n\n        elif self.model_type == 'vit':\n            # ViT expects images normalized and reshaped as batches\n            vit_outputs = self.vit(pixel_values=images)\n            features = vit_outputs.last_hidden_state[:, 0, :]  # CLS token features\n\n        features = self.fc(features)  # Project to embed_size\n        return features\n\n","metadata":{"id":"_mq_mUVUwzCC","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.717277Z","iopub.execute_input":"2024-12-21T07:14:48.717607Z","iopub.status.idle":"2024-12-21T07:14:48.733627Z","shell.execute_reply.started":"2024-12-21T07:14:48.717576Z","shell.execute_reply":"2024-12-21T07:14:48.732675Z"}},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"Decoder Part","metadata":{}},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n        super(DecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, features, captions):\n        embeddings = self.embedding(captions)\n        inputs = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n        lstm_out, _ = self.lstm(inputs)\n        output = self.fc(lstm_out)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.737448Z","iopub.execute_input":"2024-12-21T07:14:48.737843Z","iopub.status.idle":"2024-12-21T07:14:48.753722Z","shell.execute_reply.started":"2024-12-21T07:14:48.737809Z","shell.execute_reply":"2024-12-21T07:14:48.752539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n    def __init__(self,\n                 embed_size, \n                 hidden_size, \n                 vocab_size, \n                 num_heads, \n                 num_layers,\n                 ff_hidden_dim,\n                 dropout = 0.1, \n                 max_seq_length = 50):\n        super(TransformerDecoder, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size,embed_size)\n        self.positional_encoding = self._get_positional_encoding(max_seq_length,embed_size)\n\n        decoder_layer = nn.TransformerDecoderLayer(d_model = embed_size,\n                                                    nhead = num_heads,\n                                                    dim_feedforward = ff_hidden_dim,\n                                                   dropout = dropout)\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers =  num_layers)\n        self.fc_out = nn.Linear(embed_size, vocab_size)\n        self.embed_size = embed_size\n\n    def _get_positional_encoding(self, max_seq_length, embed_size):\n        \n        positional_encoding = torch.zeros(max_seq_length, embed_size)\n        \n        position = torch.arange(0,max_seq_length).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0,embed_size,2).float() * -(torch.log(torch.tensor(10000.0)) / embed_size)) # why minus sign?\n        \n        positional_encoding[:,0::2] = torch.sin(position * div_term) # why not unsquuezing or dot product\n        positional_encoding[:,1::2] = torch.cos(position * div_term) \n        positiÄ±nal_encoding = positinal_encoding.unsqueeze(0)\n       \n        return nn.Parameter(positional_encoding, requires_grad = False)\n        \n    def forward(self, features, captions):\n        \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.755048Z","iopub.execute_input":"2024-12-21T07:14:48.755527Z","iopub.status.idle":"2024-12-21T07:14:48.773460Z","shell.execute_reply.started":"2024-12-21T07:14:48.755481Z","shell.execute_reply":"2024-12-21T07:14:48.771947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Complete Model","metadata":{}},{"cell_type":"code","source":"class ImageCaptioningModel(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(ImageCaptioningModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:15:39.732035Z","iopub.execute_input":"2024-12-21T07:15:39.732490Z","iopub.status.idle":"2024-12-21T07:15:39.738443Z","shell.execute_reply.started":"2024-12-21T07:15:39.732453Z","shell.execute_reply":"2024-12-21T07:15:39.737382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# verify coco dataset\ncoco_dataset_path = '/kaggle/input/coco-2017-dataset/coco2017'\nannotation_path = os.path.join(coco_dataset_path, 'annotations')\n\n\ntrain_images_path = os.path.join(coco_dataset_path, 'train2017')\nval_images_path = os.path.join(coco_dataset_path, 'val2017')\ntest_images_path = os.path.join(coco_dataset_path, 'test2017')\n\ntrain_annotations_path = os.path.join(annotation_path, 'captions_train2017.json')\nval_annotations_path = os.path.join(annotation_path, 'captions_val2017.json')\n#test_annotations_path = os.path.join(annotation_path, 'captions_test2017.json')\n\n\n# Check if paths exist\npaths = [coco_dataset_path, train_images_path, val_images_path, test_images_path, \n         train_annotations_path, val_annotations_path]\n\nfor path in paths:\n    print(f\"{path}: {os.path.exists(path)}\")\n\n# Count number of images in each image directory\ntrain_images_count = len(os.listdir(train_images_path))\nval_images_count = len(os.listdir(val_images_path))\ntest_images_count = len(os.listdir(test_images_path))\n\n\nprint(f\"Number of training images: {train_images_count}\")\nprint(f\"Number of validation images: {val_images_count}\")\nprint(f\"Number of test images: {test_images_count}\")\n\n#TODO annotations","metadata":{"id":"hb3E8-q3wzCA","outputId":"a7b21fa8-345f-4e1f-ab01-e86c464284b5","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:15:44.735423Z","iopub.execute_input":"2024-12-21T07:15:44.735820Z","iopub.status.idle":"2024-12-21T07:15:45.973334Z","shell.execute_reply.started":"2024-12-21T07:15:44.735785Z","shell.execute_reply":"2024-12-21T07:15:45.972250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\ndef collate_fn(batch):\n    \"\"\"collates a batch of (image, caption) pairs and pads captions.\"\"\"\n    images, targets = zip(*batch)\n    image_ids = [target[\"image_id\"] for target in targets]\n    captions = [target[\"captions\"] for target in targets]\n    # Pad captions to have the same length in the batch\n    padded_captions = pad_sequence(captions, batch_first=True, padding_value=0)  # <pad> token is 0\n\n    # Stack images into a batch\n    images = torch.stack(images, 0) # dim: The dimension along which to stack. If dim=0, it adds a new first dimension to the tensors.\n   # NO NEED FOR padded_captions = torch.stack(padded_captions, 0) , padded_captions are already stacked\n\n    return images, image_ids, padded_captions","metadata":{"id":"fibMtjjW5bBm","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.783111Z","iopub.status.idle":"2024-12-21T07:14:48.783689Z","shell.execute_reply.started":"2024-12-21T07:14:48.783415Z","shell.execute_reply":"2024-12-21T07:14:48.783445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    for images, image_ids, captions in dataloader:\n        #captions = targets['captions']\n        images = images.to(device)\n        captions = captions.to(device)\n        #print(len(captions))\n        #print(len(images))\n        optimizer.zero_grad()\n        outputs = model(images, captions[:, :-1])  # Exclude last token in captions\n\n        batch_size, seq_len, vocab_size = outputs.size()\n        outputs = outputs.view(-1, vocab_size)  # Flattens outputs\n        captions = captions.view(-1)\n        loss = criterion(outputs, captions)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Print the current loss for monitoring the training progress\n    print(f\"Train Loss for current epoch is : {running_loss / len(dataloader)}\")\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for images, image_ids, captions in dataloader:\n            #captions = targets['captions']\n            images = images.to(device)\n            captions = captions.to(device)\n\n            #TODO DIMENSIONALITY PROBLEM\n            outputs = model(images, captions[:, :-1])  # Exclude last token in captions\n            batch_size, seq_len, vocab_size = outputs.size()\n            outputs = outputs.view(-1, vocab_size)  # Flattens outputs\n            captions = captions.view(-1)\n            loss = criterion(outputs, captions)  # Teacher forcing\n            total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n","metadata":{"id":"ibJfgKj2wzCC","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.785340Z","iopub.status.idle":"2024-12-21T07:14:48.785701Z","shell.execute_reply.started":"2024-12-21T07:14:48.785535Z","shell.execute_reply":"2024-12-21T07:14:48.785553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create a directory for checkpoints -> this can be used for storing state of the model in the midst of the training.\ncheckpoint_dir = 'checkpoints'\nos.makedirs(checkpoint_dir, exist_ok= True)\n\ndef save_checkpoint(model, optimizer, epoch, loss, checkpoint_path):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.787093Z","iopub.status.idle":"2024-12-21T07:14:48.787514Z","shell.execute_reply.started":"2024-12-21T07:14:48.787336Z","shell.execute_reply":"2024-12-21T07:14:48.787355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random \n\ntrain_image_number = 20000 \nval_image_number = 4000\ntest_image_number = 4000\n\n# Load COCO Dataset (captions and images) using\ntrain_dataset = CocoDataset(train_images_path, train_annotations_path, transform=transform)\n\n# Example: Fetch an image and its caption\nimage, caption = train_dataset[0]\n\nval_dataset = CocoDataset(val_images_path, val_annotations_path, transform=transform)\n\ntrain_subset_indices = random.sample(range(len(train_dataset)),train_image_number)\nremaining_indices = list(set(range(len(train_dataset))) - set(train_subset_indices))\ntest_subset_indices = random.sample(remaining_indices, test_image_number)\n\nval_subset_indices = random.sample(range(len(val_dataset)),val_image_number)\n\ntrain_subset = Subset(train_dataset, train_subset_indices)\nval_subset = Subset(val_dataset, val_subset_indices)\ntest_subset = Subset(train_dataset, test_subset_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.789442Z","iopub.status.idle":"2024-12-21T07:14:48.789992Z","shell.execute_reply.started":"2024-12-21T07:14:48.789715Z","shell.execute_reply":"2024-12-21T07:14:48.789742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# important\nyou can change the model type from the cell below, it is resnet50 here.","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nembed_size = 512\nhidden_size = 1024\nnum_epochs = 20\nlearning_rate = 0.0001\nbatch_size = 32\nnum_layers = 2\n\n# Data Loaders\ntrain_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_subset, batch_size = batch_size, shuffle = False, collate_fn = collate_fn)\n\n# Vocab\ntrain_dataset.vocab = train_dataset.build_vocab()\n# Instantiate Encoder, Decoder, and Model\nencoder = EncoderCNN(embed_size, model_type = 'resnet50').to(device)\ndecoder = DecoderRNN(embed_size, hidden_size, len(train_dataset.vocab), num_layers=num_layers).to(device) # TODO\nmodel = ImageCaptioningModel(encoder, decoder).to(device)\n\n# Optimizer and Loss Function\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\n","metadata":{"id":"jLqqdHQkwzCD","outputId":"1a25e518-6499-4b0e-93ed-426e8d1219b3","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.791333Z","iopub.status.idle":"2024-12-21T07:14:48.791889Z","shell.execute_reply.started":"2024-12-21T07:14:48.791616Z","shell.execute_reply":"2024-12-21T07:14:48.791645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Loop\nfor epoch in tqdm(range(num_epochs)):\n    train(model, train_loader, optimizer, criterion, device)\n\n    print(f\"Training for epoch {epoch+1}/{num_epochs} completed.\")\n\n    # Commented out - quicker\n    val_loss = evaluate(model, val_loader, criterion, device)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss}\")\n\n# Save the trained model\ntorch.save(model.state_dict(), 'image_captioning_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.793704Z","iopub.status.idle":"2024-12-21T07:14:48.794270Z","shell.execute_reply.started":"2024-12-21T07:14:48.793971Z","shell.execute_reply":"2024-12-21T07:14:48.793999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load('image_captioning_model.pth'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.795798Z","iopub.status.idle":"2024-12-21T07:14:48.796362Z","shell.execute_reply.started":"2024-12-21T07:14:48.796058Z","shell.execute_reply":"2024-12-21T07:14:48.796085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(image, model, vocab, device):\n\n    model.eval()\n    print(image.shape)\n    image = image.unsqueeze(0).to(device) # unsqueezing for encapsulating it inside a batch\n    print(image.shape)\n    features = model.encoder(image)\n\n    # Generate caption\n    caption = ['<bos>']\n    for _ in range(50):  # Maximum caption length\n        input_caption = torch.tensor([vocab[token] for token in caption]).unsqueeze(0).to(device)\n        outputs = model.decoder(features, input_caption)\n        _, predicted = outputs.max(2)  # outputs.shape = (batch,seq_len,vocab_size), vocab_size needed because we need to calculate each word's probabilty and pick the best of them\n        predicted_word = vocab.get_itos()[predicted[0, -1].item()] # get the first batch (only one), last predicted word in lstm(next word) convert it to string instead of an id\n        caption.append(predicted_word)\n        if predicted_word == '<eos>':\n            break\n    print(caption)\n    return ' '.join(caption[1:-1])  # Removing <bos> and <eos>\n\n# Load trained model and generate a caption\nmodel.load_state_dict(torch.load('image_captioning_model.pth'))","metadata":{"id":"plW8XdpVwzCC","outputId":"89c8a971-ad5a-4387-c743-27f072321f30","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.800128Z","iopub.status.idle":"2024-12-21T07:14:48.800543Z","shell.execute_reply.started":"2024-12-21T07:14:48.800369Z","shell.execute_reply":"2024-12-21T07:14:48.800388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_captions_for_coco(val_loader,model,vocab, max_length= 50, num_samples=5):\n    all_captions = {}\n    model.eval()\n    with torch.no_grad():\n        for i, (images, image_ids,captions) in enumerate(val_loader):\n            #image_ids = targets['image_id']\n            images = images.to(device)\n            for image_id, image in zip(image_ids,images):\n              generated_caption = generate_caption(image, model, vocab, device)\n              print(f\"type(generated_caption) = {type(generated_caption)} , {type(image_id)}\")\n              all_captions[image_id] = generated_caption\n              print(f\"Generated Caption for Image {image_id}: {generated_caption}\")\n            if i >= num_samples:\n                break\n    print(all_captions)\n    return all_captions","metadata":{"id":"6QsDeXs4wzCD","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.802139Z","iopub.status.idle":"2024-12-21T07:14:48.802523Z","shell.execute_reply.started":"2024-12-21T07:14:48.802360Z","shell.execute_reply":"2024-12-21T07:14:48.802377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef evaluate_captioning_model(generated_captions, coco_annotation_file=train_annotations_path,coco_image_dir=train_images_path):\n    \"\"\"\n    Evaluate the image captioning model using COCO evaluation metrics: BLEU, METEOR, ROUGE, CIDEr.\n\n    Parameters:\n        generated_captions (dict): Dictionary of generated captions with image_ids as keys.\n        coco_annotation_file (str): Path to COCO annotations file.\n\n    Returns:\n        dict: Dictionary containing BLEU, METEOR, ROUGE, CIDEr scores.\n    \"\"\"\n    # Load the COCO dataset annotations (reference captions)\n    coco = COCO(coco_annotation_file)\n\n    # Create a dictionary for the generated captions (image_id -> caption)\n    coco_results = [{'image_id': image_id, 'caption': caption} for image_id, caption in generated_captions.items()]\n\n\n    # Save the generated captions in a temporary file\n    with open('generated_captions.json', 'w') as f:\n        json.dump(coco_results, f)\n\n    # Load the results into COCO's evaluation API\n    coco_results = coco.loadRes('generated_captions.json')\n    #print(coco_results)\n    #print(\"Generated Captions Image IDs:\", generated_captions.keys())\n    #print(\"Ground Truth Image IDs:\", coco.getImgIds())\n\n\n    # since we filtered the images to contain only first 1000 images, lets filter the metric\n    all_image_ids = coco.getImgIds()\n    filtered_image_ids = [image_id for image_id in generated_captions.keys()]\n\n    # we need to revise the filtered version of the actual annotations\n    gts = {}\n    for image_id in filtered_image_ids:\n        caption_ids = coco.getAnnIds(imgIds=image_id)\n        annotations = coco.loadAnns(caption_ids)\n        gts[image_id] = [annotation['caption'] for annotation in annotations]\n\n\n    # Set up the evaluation\n    #print(\"gts.keys(): \",gts.keys())\n    #print(\"coco_results.keys(): \", generated_captions.keys())\n    #assert(gts.keys() == coco_results.keys())\n\n    coco_eval = COCOEvalCap(coco, coco_results)\n    coco_eval.params['image_id'] = filtered_image_ids\n    coco_eval.evaluate()\n\n    # Extract and return the metrics (BLEU, METEOR, ROUGE, CIDEr)\n    metrics = coco_eval.eval\n    return metrics\n\n# Example: Generated captions for some images\ndummy_generated_captions = {\n    12345: \"A man in a black shirt is riding a bike.\",\n    67890: \"A dog running through the grass.\",\n    11223: \"A woman holding a book in her hand.\"\n}\n\ngenerated_captions = generate_captions_for_coco(test_loader,model,vocabulary)\n# Evaluate the generated captions\nmetrics = evaluate_captioning_model(generated_captions)\n\n\nprint(\"Evaluation Metrics:\", metrics)","metadata":{"id":"JEXBUDkRwzCD","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.803789Z","iopub.status.idle":"2024-12-21T07:14:48.804238Z","shell.execute_reply.started":"2024-12-21T07:14:48.803979Z","shell.execute_reply":"2024-12-21T07:14:48.804003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom pycocotools.coco import COCO\n\ndef show_images_with_captions(generated_captions, coco_annotation_file, coco_image_dir):\n    \"\"\"\n    Display images with their generated captions.\n\n    Parameters:\n        generated_captions (dict): Dictionary of generated captions with image_ids as keys.\n        coco_annotation_file (str): Path to COCO annotations file.\n        coco_image_dir (str): Path to the directory containing COCO images.\n    \"\"\"\n    # Load the COCO dataset annotations\n    coco = COCO(coco_annotation_file)\n\n    for image_id, caption in generated_captions.items():\n        # Get image information from COCO\n        try:\n            image_info = coco.loadImgs(image_id)[0]\n            image_path = f\"{coco_image_dir}/{image_info['file_name']}\"\n            print(f\"Image path: {image_path}\")  # Debug: Print the image path\n        except KeyError:\n            print(f\"Image ID {image_id} not found in the dataset.\")\n            continue\n\n        # Load and display the image using PIL and matplotlib\n        try:\n            image = Image.open(image_path)\n        except FileNotFoundError:\n            print(f\"Image file not found at {image_path}\")\n            continue\n\n        # Print the caption\n        print(f\"Generated Caption: {caption}\")\n\n        # Display the image\n        plt.figure(figsize=(8, 8))\n        plt.imshow(image)\n        plt.axis('off')\n        plt.title(f\"Generated Caption: {caption}\", fontsize=14, wrap=True)\n        plt.show()\n\n# Generate captions for COCO validation images\ngenerated_captions = generate_captions_for_coco(val_loader, model, vocabulary)\n\n# Evaluate the generated captions\nmetrics = evaluate_captioning_model(generated_captions)\n\n# Print evaluation metrics\nprint(\"Evaluation Metrics:\", metrics)\n\n# Display images and their captions\nshow_images_with_captions(generated_captions, val_annotations_path, val_images_path)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T07:14:48.805874Z","iopub.status.idle":"2024-12-21T07:14:48.806249Z","shell.execute_reply.started":"2024-12-21T07:14:48.806045Z","shell.execute_reply":"2024-12-21T07:14:48.806061Z"}},"outputs":[],"execution_count":null}]}