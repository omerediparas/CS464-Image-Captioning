{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQnm12NRwzB1"
   },
   "source": [
    "## Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNQCuzqswzB6"
   },
   "source": [
    "\n",
    "*YAPILACAKLAR*\n",
    "\n",
    "- load_dataset iÃ§in Huggingface authentication gibi biÅŸiler istiyor, ya onu yaparÄ±z ya da farklÄ± bi library/method kullanÄ±rÄ±z.\n",
    "\n",
    "- Bir an Ã¶nce bug larÄ± Ã§Ã¶zmek lazÄ±m load_dataset e kadar sorun yok an itibariyle inÅŸ, kontrol edebilirseniz sÃ¼per olur.\n",
    "\n",
    "Yapcazzz bu iÅŸiii <3\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of Google Colab is advised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DsrE3bO_I1LN",
    "outputId": "ff99b284-5460-4ba5-d7b3-b36ac99b8f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7GRdFYeP_ZBI",
    "outputId": "d9be9b94-a48d-4fae-d18c-eb74e4f141db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Install system dependencies\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y build-essential\n",
    "!sudo apt-get install -y python3-dev\n",
    "\n",
    "# Create requirements.txt file\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write('''\n",
    "torch==2.0.0\n",
    "torchvision==0.15.1\n",
    "torchaudio\n",
    "torchtext==0.15.1\n",
    "transformers\n",
    "datasets\n",
    "Pillow\n",
    "numpy\n",
    "opencv-python-headless\n",
    "tqdm\n",
    "requests\n",
    "fsspec==2024.9.0\n",
    "''')\n",
    "    #git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI ultralytics==8.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KQUcn5dAEwWV"
   },
   "outputs": [],
   "source": [
    "#rm -rf /content/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xATJJqdWwzB-",
    "outputId": "b425b4ba-5ed6-47aa-bd22-c7ba69077b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: torchvision==0.15.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.15.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.0.1)\n",
      "Requirement already satisfied: torchtext==0.15.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.15.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.46.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (11.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.10.0.84)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.66.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
      "Requirement already satisfied: fsspec==2024.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (2024.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 2)) (75.1.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 2)) (0.44.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0->torchtext==0.15.1->-r requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 2)) (3.30.5)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 2)) (18.1.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (0.20.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (3.10.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 12)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 12)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 12)) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->-r requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->-r requirements.txt (line 7)) (0.2.0)\n",
      "Collecting pycocotools\n",
      "  Cloning https://github.com/philferriere/cocoapi.git to /tmp/pip-install-xn16qmci/pycocotools_5c029779dc424f4da3bb9f4aa14bffa2\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/philferriere/cocoapi.git /tmp/pip-install-xn16qmci/pycocotools_5c029779dc424f4da3bb9f4aa14bffa2\n",
      "  Resolved https://github.com/philferriere/cocoapi.git to commit 2929bd2ef6b451054755dfd7ceb09278f935f7ad\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/ultralytics/yolov5\n",
    "\n",
    "#Uncomment below if you are using Google Colab.\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "#!pip install \"git+https://github.com/salaniz/pycocoevalcap.git\"\n",
    "!pip3 install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\"\n",
    "#!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
    "#!pip install ultralytics==8.0.3  # Specify a stable YOLOv5 version\n",
    "#!pip install git+https://github.com/ultralytics/yolov5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SU2rJWpcrkaF",
    "outputId": "dcecdbd0-06b3-487e-a7a9-64fffb6b703c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov5' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/ultralytics/yolov5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJCjC8i4_JTq"
   },
   "source": [
    "# Inital Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V1LDt60jwzB7"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import pycocoevalcap\n",
    "from ultralytics import YOLO\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from collections import Counter\n",
    "# for metrics, COCO API metrics is used\n",
    "import json\n",
    "from pycocoevalcap import bleu, meteor, rouge, spice\n",
    "#from pycocoevalcap.evalcap import COCOEvalCap\n",
    "\n",
    "# Load YOLOv5 for feature extraction\n",
    "import torch.hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2-uX4UDUw3Nt"
   },
   "outputs": [],
   "source": [
    "# setting the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8emL2iooGE4O"
   },
   "source": [
    "Pre-Trained Encoder (Yolo-v5) ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6bkn5ExuLLA",
    "outputId": "da511454-f90e-40ac-a2d5-9660dce0cfe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/yolov5\n"
     ]
    }
   ],
   "source": [
    "\"\"\" -NOT NEEDED ANYMORE- This is for local storage of the YOLO weights, we are uploading the model from github with the next code block.\n",
    "%cd yolov5\n",
    "!wget https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt # get the pre-trained weights\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIzkhVwfwzB_",
    "outputId": "ac624a36-cc88-4871-b617-e58c060acb33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-11-13 Python-3.10.12 torch-2.0.0+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained YOLOv5 model from Torch Hub\n",
    "yolov5 = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).eval().to(device)\n",
    "\n",
    "#This function pulls models from GitHub repositories and is commonly used for loading pre-trained models. When you specify 'ultralytics/yolov5', it attempts to fetch the YOLOv5 repository from GitHub.\n",
    "\n",
    "\n",
    "#model = YOLO('yolov8s.pt').to('cuda') if torch.cuda.is_available() else YOLO('yolov8s.pt')\n",
    "#yolov5s.pt, yolov5m.pt, yolov5l.pt, yolov5x.pt are pretrained YOLOv5 models with different sizes (s for small, m for medium, l for large, x for extra-large).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aiAW9oHwzB_",
    "outputId": "fc317257-3cef-4b7d-b367-973c9906e9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'coco # load evertything inside coco folder'\n",
      "/content\n",
      "--2024-11-13 20:11:42--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.27.137, 3.5.29.195, 52.217.205.73, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.27.137|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: â€˜annotations_trainval2017.zipâ€™\n",
      "\n",
      "annotations_trainva 100%[===================>] 241.19M  17.1MB/s    in 17s     \n",
      "\n",
      "2024-11-13 20:11:59 (14.6 MB/s) - â€˜annotations_trainval2017.zipâ€™ saved [252907541/252907541]\n",
      "\n",
      "Archive:  annotations_trainval2017.zip\n",
      "  inflating: annotations/instances_train2017.json  \n",
      "  inflating: annotations/instances_val2017.json  \n",
      "  inflating: annotations/captions_train2017.json  \n",
      "  inflating: annotations/captions_val2017.json  \n",
      "  inflating: annotations/person_keypoints_train2017.json  \n",
      "  inflating: annotations/person_keypoints_val2017.json  \n",
      "--2024-11-13 20:12:17--  http://images.cocodataset.org/zips/train2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.27.100, 16.182.69.233, 52.216.33.241, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.27.100|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19336861798 (18G) [application/zip]\n",
      "Saving to: â€˜train2017.zipâ€™\n",
      "\n",
      "train2017.zip         6%[>                   ]   1.12G  17.4MB/s    eta 16m 36s^C\n",
      "Archive:  train2017.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of train2017.zip or\n",
      "        train2017.zip.zip, and cannot find train2017.zip.ZIP, period.\n",
      "--2024-11-13 20:13:24--  http://images.cocodataset.org/zips/val2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.171.57, 3.5.25.213, 3.5.27.19, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.171.57|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 815585330 (778M) [application/zip]\n",
      "Saving to: â€˜val2017.zipâ€™\n",
      "\n",
      "val2017.zip           0%[                    ] 875.03K   771KB/s               ^C\n",
      "Archive:  val2017.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of val2017.zip or\n",
      "        val2017.zip.zip, and cannot find val2017.zip.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "# Download Dataset\n",
    "%mkdir -p coco\n",
    "%cd coco  # load evertything inside 'coco' folder\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip annotations_trainval2017.zip\n",
    "!wget http://images.cocodataset.org/zips/train2017.zip\n",
    "!unzip train2017.zip\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip val2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZD4nNkbRluEG",
    "outputId": "c4186428-cfb9-4e96-98c6-b534fccd9192"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ultralytics.yolo.engine.model.YOLO at 0x79d024192440>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pdEPXli8wzCA"
   },
   "outputs": [],
   "source": [
    "# Define transformations for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "hb3E8-q3wzCA",
    "outputId": "be8b1b28-075f-4e4b-c4ea-e370cbea81ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'coco' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9e5e93b66eda>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0;31m# Load the training split of the COCO dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coco\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the validation split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2132\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1641\u001b[0m                 ) from e\n\u001b[1;32m   1642\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1644\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 dataset_script_path = api.hf_hub_download(\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'coco' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "# Load COCO Dataset (captions and images)\n",
    "\n",
    "# The COCO dataset is already pre-split into train, validation, and test subsets by its creators, and Hugging Face provides access to these splits directly.\n",
    "\n",
    " # Load the training split of the COCO dataset\n",
    "train_dataset = load_dataset(\"coco\", split='train')\n",
    "\n",
    "# Load the validation split\n",
    "val_dataset = load_dataset(\"coco\", split='validation')\n",
    "\n",
    "# Load the test split\n",
    "test_dataset = load_dataset(\"coco\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_cYJu5QwzCB"
   },
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset): # <start> cat sat on the mat <end> -> {0 32 24 34 3 3 1 }\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.annotations = load_dataset('json', data_files=annotation_file)['train']\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")  # Tokenizer from torchtext\n",
    "\n",
    "        # Build vocabulary\n",
    "        self.vocab = self.build_vocab()\n",
    "\n",
    "    def build_vocab(self):\n",
    "        counter = Counter()\n",
    "        for annotation in tqdm(self.annotations):\n",
    "            caption = annotation['caption']\n",
    "            tokens = self.tokenizer(caption.lower())\n",
    "            counter.update(tokens)\n",
    "        vocab = build_vocab_from_iterator([counter], specials=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"])\n",
    "        vocab.set_default_index(vocab[\"<unk>\"])  # Out-of-vocabulary words will be mapped to <unk>\n",
    "        return vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations[idx]['file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        caption = self.annotations[idx]['caption']\n",
    "        tokens = self.tokenizer(caption.lower())\n",
    "\n",
    "        # Convert caption tokens to indices\n",
    "        caption_indices = [self.vocab['<bos>']] + [self.vocab[token] for token in tokens] + [self.vocab['<eos>']]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(caption_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mq_mUVUwzCC"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.cnn = yolov5.models.common.AutoShape(yolov5.load(\"yolov5s\"))\n",
    "        self.fc = nn.Linear(1000, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.cnn(images)  # Get features from YOLOv5\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,batch_first=True)#, bi-directional=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding(captions)\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibJfgKj2wzCC"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for images, captions in dataloader:\n",
    "        images = torch.stack(images).to(device)\n",
    "        captions = torch.stack(captions).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, captions[:, :-1])  # Exclude last token in captions\n",
    "        loss = criterion(outputs.view(-1, outputs.size(2)), captions[:, 1:].contiguous().view(-1))  # Teacher forcing\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images = torch.stack(images).to(device)\n",
    "            captions = torch.stack(captions).to(device)\n",
    "\n",
    "            outputs = model(images, captions[:, :-1])  # Exclude last token in captions\n",
    "            loss = criterion(outputs.view(-1, outputs.size(2)), captions[:, 1:].contiguous().view(-1))  # Teacher forcing\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plW8XdpVwzCC"
   },
   "outputs": [],
   "source": [
    "def generate_caption(image, model, vocab, device):\n",
    "    model.eval()\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    features = model.encoder(image)\n",
    "\n",
    "    # Generate caption\n",
    "    caption = ['<bos>']\n",
    "    for _ in range(50):  # Maximum caption length\n",
    "        input_caption = torch.tensor([vocab[token] for token in caption]).unsqueeze(0).to(device)\n",
    "        outputs = model.decoder(features, input_caption)\n",
    "        _, predicted = outputs.max(2)\n",
    "        predicted_word = vocab.lookup_token(predicted[0, -1].item())\n",
    "        caption.append(predicted_word)\n",
    "        if predicted_word == '<eos>':\n",
    "            break\n",
    "\n",
    "    return ' '.join(caption[1:-1])  # Removing <bos> and <eos>\n",
    "\n",
    "# Load trained model and generate a caption\n",
    "model.load_state_dict(torch.load('image_captioning_model.pth'))\n",
    "image = Image.open(\"path_to_image.jpg\")  # Replace with your image path\n",
    "caption = generate_caption(image, model, train_dataset.vocab, device)\n",
    "print(f\"Generated Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLqqdHQkwzCD"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate Encoder, Decoder, and Model\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(train_dataset.vocab)).to(device)\n",
    "model = ImageCaptioningModel(encoder, decoder).to(3)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'image_captioning_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QsDeXs4wzCD"
   },
   "outputs": [],
   "source": [
    "caption = generate_caption(encoder, decoder, 'path/to/your/image.jpg', vocab, transform, device)\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEXBUDkRwzCD"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_captioning_model(generated_captions, coco_annotation_file='annotations/captions_val2017.json'):\n",
    "    \"\"\"\n",
    "    Evaluate the image captioning model using COCO evaluation metrics: BLEU, METEOR, ROUGE, CIDEr.\n",
    "\n",
    "    Parameters:\n",
    "        generated_captions (dict): Dictionary of generated captions with image_ids as keys.\n",
    "        coco_annotation_file (str): Path to COCO annotations file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing BLEU, METEOR, ROUGE, CIDEr scores.\n",
    "    \"\"\"\n",
    "    # Load the COCO dataset annotations (reference captions)\n",
    "    coco = COCO(coco_annotation_file)\n",
    "\n",
    "    # Create a dictionary for the generated captions (image_id -> caption)\n",
    "    coco_results = []\n",
    "    for image_id, caption in generated_captions.items():\n",
    "        coco_results.append({\n",
    "            'image_id': image_id,\n",
    "            'caption': caption\n",
    "        })\n",
    "\n",
    "    # Save the generated captions in a temporary file\n",
    "    with open('generated_captions.json', 'w') as f:\n",
    "        json.dump(coco_results, f)\n",
    "\n",
    "    # Load the results into COCO's evaluation API\n",
    "    coco_results = coco.loadRes('generated_captions.json')\n",
    "\n",
    "    # Set up the evaluation\n",
    "    coco_eval = COCOEvalCap(coco, coco_results)\n",
    "    coco_eval.evaluate()\n",
    "\n",
    "    # Extract and return the metrics (BLEU, METEOR, ROUGE, CIDEr)\n",
    "    metrics = coco_eval.eval\n",
    "    return metrics\n",
    "\n",
    "# Example: Generated captions for some images\n",
    "generated_captions = {\n",
    "    12345: \"A man in a black shirt is riding a bike.\",\n",
    "    67890: \"A dog running through the grass.\",\n",
    "    11223: \"A woman holding a book in her hand.\"\n",
    "}\n",
    "\n",
    "# Evaluate the generated captions\n",
    "metrics = evaluate_captioning_model(generated_captions)\n",
    "print(\"Evaluation Metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
